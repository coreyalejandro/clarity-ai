{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 4.790123456790123,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04938271604938271,
      "grad_norm": 57.35495376586914,
      "learning_rate": 5e-05,
      "loss": 6.4105,
      "step": 1
    },
    {
      "epoch": 0.09876543209876543,
      "grad_norm": 37.626869201660156,
      "learning_rate": 4.9523809523809525e-05,
      "loss": 4.5562,
      "step": 2
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 36.43342590332031,
      "learning_rate": 4.904761904761905e-05,
      "loss": 4.4397,
      "step": 3
    },
    {
      "epoch": 0.19753086419753085,
      "grad_norm": 43.127952575683594,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 4.2346,
      "step": 4
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 35.78925323486328,
      "learning_rate": 4.80952380952381e-05,
      "loss": 3.8164,
      "step": 5
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 34.82102584838867,
      "learning_rate": 4.761904761904762e-05,
      "loss": 3.2679,
      "step": 6
    },
    {
      "epoch": 0.345679012345679,
      "grad_norm": 37.560997009277344,
      "learning_rate": 4.714285714285714e-05,
      "loss": 3.16,
      "step": 7
    },
    {
      "epoch": 0.3950617283950617,
      "grad_norm": 35.73603820800781,
      "learning_rate": 4.666666666666667e-05,
      "loss": 3.7172,
      "step": 8
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 30.647470474243164,
      "learning_rate": 4.6190476190476194e-05,
      "loss": 3.5532,
      "step": 9
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 28.85980987548828,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 2.801,
      "step": 10
    },
    {
      "epoch": 0.5432098765432098,
      "grad_norm": 27.100412368774414,
      "learning_rate": 4.523809523809524e-05,
      "loss": 2.6253,
      "step": 11
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 22.27338981628418,
      "learning_rate": 4.476190476190477e-05,
      "loss": 2.3196,
      "step": 12
    },
    {
      "epoch": 0.6419753086419753,
      "grad_norm": 25.908666610717773,
      "learning_rate": 4.428571428571428e-05,
      "loss": 2.3595,
      "step": 13
    },
    {
      "epoch": 0.691358024691358,
      "grad_norm": 27.974233627319336,
      "learning_rate": 4.380952380952381e-05,
      "loss": 2.7755,
      "step": 14
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 20.890729904174805,
      "learning_rate": 4.3333333333333334e-05,
      "loss": 1.8258,
      "step": 15
    },
    {
      "epoch": 0.7901234567901234,
      "grad_norm": 26.886844635009766,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 2.1868,
      "step": 16
    },
    {
      "epoch": 0.8395061728395061,
      "grad_norm": 25.61473274230957,
      "learning_rate": 4.2380952380952385e-05,
      "loss": 2.1133,
      "step": 17
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 28.8938045501709,
      "learning_rate": 4.190476190476191e-05,
      "loss": 2.3121,
      "step": 18
    },
    {
      "epoch": 0.9382716049382716,
      "grad_norm": 25.556215286254883,
      "learning_rate": 4.1428571428571437e-05,
      "loss": 2.0846,
      "step": 19
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 31.18901252746582,
      "learning_rate": 4.095238095238095e-05,
      "loss": 2.3149,
      "step": 20
    },
    {
      "epoch": 1.0,
      "grad_norm": 49.63093566894531,
      "learning_rate": 4.047619047619048e-05,
      "loss": 1.8944,
      "step": 21
    },
    {
      "epoch": 1.0493827160493827,
      "grad_norm": 22.436920166015625,
      "learning_rate": 4e-05,
      "loss": 1.8885,
      "step": 22
    },
    {
      "epoch": 1.0987654320987654,
      "grad_norm": 27.039960861206055,
      "learning_rate": 3.9523809523809526e-05,
      "loss": 1.9495,
      "step": 23
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 24.564090728759766,
      "learning_rate": 3.904761904761905e-05,
      "loss": 2.2496,
      "step": 24
    },
    {
      "epoch": 1.1975308641975309,
      "grad_norm": 21.64994239807129,
      "learning_rate": 3.857142857142858e-05,
      "loss": 1.9372,
      "step": 25
    },
    {
      "epoch": 1.2469135802469136,
      "grad_norm": 21.11888313293457,
      "learning_rate": 3.809523809523809e-05,
      "loss": 1.5941,
      "step": 26
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 22.709293365478516,
      "learning_rate": 3.761904761904762e-05,
      "loss": 1.7091,
      "step": 27
    },
    {
      "epoch": 1.345679012345679,
      "grad_norm": 24.90890121459961,
      "learning_rate": 3.7142857142857143e-05,
      "loss": 1.6299,
      "step": 28
    },
    {
      "epoch": 1.3950617283950617,
      "grad_norm": 28.136404037475586,
      "learning_rate": 3.6666666666666666e-05,
      "loss": 1.6057,
      "step": 29
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 21.769502639770508,
      "learning_rate": 3.619047619047619e-05,
      "loss": 1.5834,
      "step": 30
    },
    {
      "epoch": 1.4938271604938271,
      "grad_norm": 23.917076110839844,
      "learning_rate": 3.571428571428572e-05,
      "loss": 1.3433,
      "step": 31
    },
    {
      "epoch": 1.5432098765432098,
      "grad_norm": 26.402565002441406,
      "learning_rate": 3.523809523809524e-05,
      "loss": 1.7508,
      "step": 32
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 28.643278121948242,
      "learning_rate": 3.476190476190476e-05,
      "loss": 1.7813,
      "step": 33
    },
    {
      "epoch": 1.6419753086419753,
      "grad_norm": 24.320587158203125,
      "learning_rate": 3.428571428571429e-05,
      "loss": 1.7174,
      "step": 34
    },
    {
      "epoch": 1.691358024691358,
      "grad_norm": 28.579147338867188,
      "learning_rate": 3.380952380952381e-05,
      "loss": 1.8001,
      "step": 35
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 24.166133880615234,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.5873,
      "step": 36
    },
    {
      "epoch": 1.7901234567901234,
      "grad_norm": 28.040328979492188,
      "learning_rate": 3.285714285714286e-05,
      "loss": 1.5524,
      "step": 37
    },
    {
      "epoch": 1.8395061728395061,
      "grad_norm": 29.15969467163086,
      "learning_rate": 3.2380952380952386e-05,
      "loss": 1.6378,
      "step": 38
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 26.68362808227539,
      "learning_rate": 3.19047619047619e-05,
      "loss": 1.6142,
      "step": 39
    },
    {
      "epoch": 1.9382716049382716,
      "grad_norm": 35.83200454711914,
      "learning_rate": 3.142857142857143e-05,
      "loss": 1.4571,
      "step": 40
    },
    {
      "epoch": 1.9876543209876543,
      "grad_norm": 25.419044494628906,
      "learning_rate": 3.095238095238095e-05,
      "loss": 1.536,
      "step": 41
    },
    {
      "epoch": 2.0,
      "grad_norm": 56.079837799072266,
      "learning_rate": 3.0476190476190482e-05,
      "loss": 1.799,
      "step": 42
    },
    {
      "epoch": 2.049382716049383,
      "grad_norm": 28.100828170776367,
      "learning_rate": 3e-05,
      "loss": 1.8318,
      "step": 43
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 24.017274856567383,
      "learning_rate": 2.9523809523809526e-05,
      "loss": 1.5185,
      "step": 44
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 19.346240997314453,
      "learning_rate": 2.9047619047619052e-05,
      "loss": 1.1945,
      "step": 45
    },
    {
      "epoch": 2.197530864197531,
      "grad_norm": 21.354999542236328,
      "learning_rate": 2.857142857142857e-05,
      "loss": 1.3391,
      "step": 46
    },
    {
      "epoch": 2.246913580246914,
      "grad_norm": 24.987028121948242,
      "learning_rate": 2.8095238095238096e-05,
      "loss": 1.6093,
      "step": 47
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 29.024051666259766,
      "learning_rate": 2.7619047619047622e-05,
      "loss": 1.9669,
      "step": 48
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 19.944751739501953,
      "learning_rate": 2.714285714285714e-05,
      "loss": 1.2352,
      "step": 49
    },
    {
      "epoch": 2.3950617283950617,
      "grad_norm": 31.471813201904297,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 1.5461,
      "step": 50
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 28.71562957763672,
      "learning_rate": 2.6190476190476192e-05,
      "loss": 1.5412,
      "step": 51
    },
    {
      "epoch": 2.493827160493827,
      "grad_norm": 19.205751419067383,
      "learning_rate": 2.5714285714285714e-05,
      "loss": 1.0635,
      "step": 52
    },
    {
      "epoch": 2.5432098765432096,
      "grad_norm": 21.660171508789062,
      "learning_rate": 2.523809523809524e-05,
      "loss": 1.2144,
      "step": 53
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 19.190221786499023,
      "learning_rate": 2.4761904761904762e-05,
      "loss": 1.478,
      "step": 54
    },
    {
      "epoch": 2.6419753086419755,
      "grad_norm": 25.352909088134766,
      "learning_rate": 2.4285714285714288e-05,
      "loss": 1.694,
      "step": 55
    },
    {
      "epoch": 2.691358024691358,
      "grad_norm": 23.198991775512695,
      "learning_rate": 2.380952380952381e-05,
      "loss": 1.3933,
      "step": 56
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 22.854909896850586,
      "learning_rate": 2.3333333333333336e-05,
      "loss": 1.3125,
      "step": 57
    },
    {
      "epoch": 2.7901234567901234,
      "grad_norm": 23.708980560302734,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 1.273,
      "step": 58
    },
    {
      "epoch": 2.8395061728395063,
      "grad_norm": 25.037485122680664,
      "learning_rate": 2.2380952380952384e-05,
      "loss": 1.3779,
      "step": 59
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 21.971803665161133,
      "learning_rate": 2.1904761904761906e-05,
      "loss": 1.4382,
      "step": 60
    },
    {
      "epoch": 2.9382716049382713,
      "grad_norm": 20.083467483520508,
      "learning_rate": 2.1428571428571428e-05,
      "loss": 1.1398,
      "step": 61
    },
    {
      "epoch": 2.9876543209876543,
      "grad_norm": 29.48300552368164,
      "learning_rate": 2.0952380952380954e-05,
      "loss": 1.5034,
      "step": 62
    },
    {
      "epoch": 3.0,
      "grad_norm": 54.12336730957031,
      "learning_rate": 2.0476190476190476e-05,
      "loss": 1.1879,
      "step": 63
    },
    {
      "epoch": 3.049382716049383,
      "grad_norm": 23.293027877807617,
      "learning_rate": 2e-05,
      "loss": 1.0288,
      "step": 64
    },
    {
      "epoch": 3.0987654320987654,
      "grad_norm": 25.834983825683594,
      "learning_rate": 1.9523809523809524e-05,
      "loss": 1.1242,
      "step": 65
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 22.248699188232422,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 0.9642,
      "step": 66
    },
    {
      "epoch": 3.197530864197531,
      "grad_norm": 21.568166732788086,
      "learning_rate": 1.8571428571428572e-05,
      "loss": 1.3099,
      "step": 67
    },
    {
      "epoch": 3.246913580246914,
      "grad_norm": 21.791006088256836,
      "learning_rate": 1.8095238095238094e-05,
      "loss": 1.0151,
      "step": 68
    },
    {
      "epoch": 3.2962962962962963,
      "grad_norm": 22.205965042114258,
      "learning_rate": 1.761904761904762e-05,
      "loss": 0.8858,
      "step": 69
    },
    {
      "epoch": 3.3456790123456788,
      "grad_norm": 26.66847801208496,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 1.2515,
      "step": 70
    },
    {
      "epoch": 3.3950617283950617,
      "grad_norm": 25.535274505615234,
      "learning_rate": 1.6666666666666667e-05,
      "loss": 1.629,
      "step": 71
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 25.82344627380371,
      "learning_rate": 1.6190476190476193e-05,
      "loss": 1.522,
      "step": 72
    },
    {
      "epoch": 3.493827160493827,
      "grad_norm": 25.332242965698242,
      "learning_rate": 1.5714285714285715e-05,
      "loss": 1.2356,
      "step": 73
    },
    {
      "epoch": 3.5432098765432096,
      "grad_norm": 27.892887115478516,
      "learning_rate": 1.5238095238095241e-05,
      "loss": 1.4512,
      "step": 74
    },
    {
      "epoch": 3.5925925925925926,
      "grad_norm": 15.599027633666992,
      "learning_rate": 1.4761904761904763e-05,
      "loss": 0.7853,
      "step": 75
    },
    {
      "epoch": 3.6419753086419755,
      "grad_norm": 28.998157501220703,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 1.5997,
      "step": 76
    },
    {
      "epoch": 3.691358024691358,
      "grad_norm": 26.161821365356445,
      "learning_rate": 1.3809523809523811e-05,
      "loss": 1.2368,
      "step": 77
    },
    {
      "epoch": 3.7407407407407405,
      "grad_norm": 23.36812973022461,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.2888,
      "step": 78
    },
    {
      "epoch": 3.7901234567901234,
      "grad_norm": 26.62158203125,
      "learning_rate": 1.2857142857142857e-05,
      "loss": 1.6755,
      "step": 79
    },
    {
      "epoch": 3.8395061728395063,
      "grad_norm": 25.123506546020508,
      "learning_rate": 1.2380952380952381e-05,
      "loss": 1.2745,
      "step": 80
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 20.896682739257812,
      "learning_rate": 1.1904761904761905e-05,
      "loss": 1.1465,
      "step": 81
    },
    {
      "epoch": 3.9382716049382713,
      "grad_norm": 29.14289665222168,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 1.0237,
      "step": 82
    },
    {
      "epoch": 3.9876543209876543,
      "grad_norm": 30.03955841064453,
      "learning_rate": 1.0952380952380953e-05,
      "loss": 1.3988,
      "step": 83
    },
    {
      "epoch": 4.0,
      "grad_norm": 48.10718536376953,
      "learning_rate": 1.0476190476190477e-05,
      "loss": 1.4093,
      "step": 84
    },
    {
      "epoch": 4.049382716049383,
      "grad_norm": 32.24872970581055,
      "learning_rate": 1e-05,
      "loss": 1.1447,
      "step": 85
    },
    {
      "epoch": 4.098765432098766,
      "grad_norm": 22.631141662597656,
      "learning_rate": 9.523809523809523e-06,
      "loss": 1.2006,
      "step": 86
    },
    {
      "epoch": 4.148148148148148,
      "grad_norm": 20.245323181152344,
      "learning_rate": 9.047619047619047e-06,
      "loss": 0.9517,
      "step": 87
    },
    {
      "epoch": 4.197530864197531,
      "grad_norm": 33.61763381958008,
      "learning_rate": 8.571428571428573e-06,
      "loss": 1.431,
      "step": 88
    },
    {
      "epoch": 4.246913580246914,
      "grad_norm": 22.345964431762695,
      "learning_rate": 8.095238095238097e-06,
      "loss": 0.9641,
      "step": 89
    },
    {
      "epoch": 4.296296296296296,
      "grad_norm": 23.925546646118164,
      "learning_rate": 7.6190476190476205e-06,
      "loss": 1.3647,
      "step": 90
    },
    {
      "epoch": 4.345679012345679,
      "grad_norm": 28.034374237060547,
      "learning_rate": 7.142857142857143e-06,
      "loss": 1.2628,
      "step": 91
    },
    {
      "epoch": 4.395061728395062,
      "grad_norm": 28.15313720703125,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.0645,
      "step": 92
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 22.807016372680664,
      "learning_rate": 6.190476190476191e-06,
      "loss": 1.4608,
      "step": 93
    },
    {
      "epoch": 4.493827160493828,
      "grad_norm": 26.420347213745117,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 1.1889,
      "step": 94
    },
    {
      "epoch": 4.54320987654321,
      "grad_norm": 23.311145782470703,
      "learning_rate": 5.2380952380952384e-06,
      "loss": 1.2732,
      "step": 95
    },
    {
      "epoch": 4.592592592592593,
      "grad_norm": 25.399198532104492,
      "learning_rate": 4.7619047619047615e-06,
      "loss": 1.0265,
      "step": 96
    },
    {
      "epoch": 4.6419753086419755,
      "grad_norm": 20.7515811920166,
      "learning_rate": 4.285714285714286e-06,
      "loss": 0.7328,
      "step": 97
    },
    {
      "epoch": 4.6913580246913575,
      "grad_norm": 20.34731101989746,
      "learning_rate": 3.8095238095238102e-06,
      "loss": 1.3874,
      "step": 98
    },
    {
      "epoch": 4.7407407407407405,
      "grad_norm": 27.982995986938477,
      "learning_rate": 3.3333333333333333e-06,
      "loss": 0.9563,
      "step": 99
    },
    {
      "epoch": 4.790123456790123,
      "grad_norm": 22.499435424804688,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 1.199,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 105,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 101381308416000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
