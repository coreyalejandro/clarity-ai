#!/usr/bin/env python3
"""
Advanced ClarityAI Demo - Showcasing Sophisticated Rubric Evaluation

This demo demonstrates how ClarityAI makes fine-tuning accessible while providing
enterprise-grade rubric evaluation with detailed explanations and actionable feedback.
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from clarity.scorer import Template
import json


def print_separator(title: str):
    """Print a formatted section separator."""
    print("\n" + "=" * 80)
    print(f" {title}")
    print("=" * 80)


def print_rule_explanation(explanation: dict):
    """Print a detailed rule explanation."""
    print(f"\nğŸ“Š {explanation['rule_type'].upper()} (Weight: {explanation['weight']})")
    print(f"   Score: {explanation['raw_score']:.3f} â†’ Weighted: {explanation['weighted_score']:.3f}")
    print(f"   Reasoning: {explanation['reasoning']}")
    
    if explanation.get('evidence'):
        print("   Evidence:")
        for evidence in explanation['evidence']:
            print(f"     â€¢ {evidence}")
    
    if explanation.get('suggestions'):
        print("   Suggestions:")
        for suggestion in explanation['suggestions']:
            print(f"     â†’ {suggestion}")
    
    print(f"   Confidence: {explanation.get('confidence', 0.8):.1%}")


def demo_academic_paper():
    """Demonstrate academic paper evaluation."""
    print_separator("ACADEMIC PAPER EVALUATION DEMO")
    
    # Load the academic paper template
    try:
        template = Template.from_yaml("templates/academic-paper.yaml")
        print(f"âœ… Loaded template: {template.name}")
        print(f"   Description: {template.description}")
        print(f"   Rules: {len(template.rules)}")
    except Exception as e:
        print(f"âŒ Error loading template: {e}")
        return
    
    # Sample academic text (intentionally mixed quality)
    academic_text = """
    Machine learning has revolutionized many fields through its ability to learn patterns from data.
    Neural networks, particularly deep learning architectures, have shown remarkable performance
    in tasks such as image recognition and natural language processing. However, these models
    often suffer from overfitting when training data is limited.
    
    To address this issue, researchers have developed various regularization techniques including
    dropout, batch normalization, and weight decay. Cross-validation is commonly used to evaluate
    model performance and select optimal hyperparameters. The gradient descent optimization
    algorithm, combined with backpropagation, enables efficient training of neural networks.
    
    Recent advances in transformer architectures and attention mechanisms have further improved
    model performance across various domains. These developments demonstrate the importance of
    feature engineering and proper model selection in achieving state-of-the-art results.
    """
    
    print(f"\nğŸ“ Sample Text ({len(academic_text.split())} words):")
    print(academic_text[:200] + "..." if len(academic_text) > 200 else academic_text)
    
    # Evaluate with explanations
    try:
        result = template.evaluate_with_explanations(academic_text)
        
        print(f"\nğŸ¯ OVERALL SCORE: {result['total_score']:.3f}")
        print(f"   {result['overall_feedback']['score_interpretation']}")
        
        print("\nğŸ“‹ DETAILED RULE BREAKDOWN:")
        for explanation in result['rule_explanations']:
            if 'error' not in explanation:
                print_rule_explanation(explanation)
            else:
                print(f"\nâŒ {explanation['rule_type']}: {explanation['error']}")
        
        print("\nğŸ’ª STRENGTHS:")
        for strength in result['overall_feedback']['strengths']:
            print(f"   âœ… {strength}")
        
        print("\nâš ï¸  AREAS FOR IMPROVEMENT:")
        for weakness in result['overall_feedback']['weaknesses']:
            print(f"   ğŸ”¸ {weakness}")
        
        print("\nğŸ’¡ ACTIONABLE SUGGESTIONS:")
        for suggestion in result['overall_feedback']['suggestions']:
            print(f"   â†’ {suggestion}")
            
    except Exception as e:
        print(f"âŒ Evaluation error: {e}")


def demo_code_review():
    """Demonstrate code review evaluation."""
    print_separator("CODE REVIEW EVALUATION DEMO")
    
    # Load the code review template
    try:
        template = Template.from_yaml("templates/code-review.yaml")
        print(f"âœ… Loaded template: {template.name}")
        print(f"   Description: {template.description}")
        print(f"   Rules: {len(template.rules)}")
    except Exception as e:
        print(f"âŒ Error loading template: {e}")
        return
    
    # Sample code review comments (good and bad examples)
    good_review = """
    I suggest refactoring the authentication logic in line 45 to improve maintainability.
    Consider implementing the Strategy pattern here to handle different authentication methods.
    The current approach creates technical debt and makes unit testing more difficult.
    
    Additionally, I recommend adding input validation for the user credentials to prevent
    security vulnerabilities. The current implementation doesn't sanitize user input,
    which could lead to injection attacks.
    """
    
    bad_review = """
    This code is bad. Fix it.
    """
    
    print("\nğŸ“ GOOD REVIEW EXAMPLE:")
    print(good_review)
    
    try:
        result = template.evaluate_with_explanations(good_review)
        print(f"\nğŸ¯ SCORE: {result['total_score']:.3f} - {result['overall_feedback']['score_interpretation']}")
        
        print("\nğŸ“Š TOP PERFORMING RULES:")
        sorted_rules = sorted(result['rule_explanations'], 
                            key=lambda x: x.get('raw_score', 0), reverse=True)[:3]
        for explanation in sorted_rules:
            if 'error' not in explanation and explanation['raw_score'] > 0.5:
                print(f"   âœ… {explanation['rule_type']}: {explanation['raw_score']:.3f}")
                print(f"      {explanation['reasoning']}")
    except Exception as e:
        print(f"âŒ Evaluation error: {e}")
    
    print("\nğŸ“ BAD REVIEW EXAMPLE:")
    print(bad_review)
    
    try:
        result = template.evaluate_with_explanations(bad_review)
        print(f"\nğŸ¯ SCORE: {result['total_score']:.3f} - {result['overall_feedback']['score_interpretation']}")
        
        print("\nâš ï¸  KEY ISSUES:")
        for weakness in result['overall_feedback']['weaknesses'][:3]:
            print(f"   ğŸ”¸ {weakness}")
        
        print("\nğŸ’¡ IMPROVEMENT SUGGESTIONS:")
        for suggestion in result['overall_feedback']['suggestions'][:5]:
            print(f"   â†’ {suggestion}")
    except Exception as e:
        print(f"âŒ Evaluation error: {e}")


def demo_fine_tuning_accessibility():
    """Demonstrate how ClarityAI makes fine-tuning accessible."""
    print_separator("FINE-TUNING ACCESSIBILITY DEMONSTRATION")
    
    print("""
ğŸ¯ THE PROBLEM WITH TRADITIONAL FINE-TUNING:
   â€¢ Requires deep ML expertise to design reward functions
   â€¢ Complex code to implement RLHF/PPO training loops  
   â€¢ No interpretable feedback on why models improve/fail
   â€¢ Expensive trial-and-error cycles
   â€¢ Black box optimization with unclear objectives

âœ¨ CLARITYAI'S SOLUTION:
   â€¢ Teacher-like rubrics anyone can understand and create
   â€¢ Visual interface for non-technical users
   â€¢ Immediate feedback with detailed rule breakdowns
   â€¢ Iterative improvement through template refinement
   â€¢ Explainable AI with actionable suggestions

ğŸ“ˆ BUSINESS IMPACT:
   â€¢ Reduces fine-tuning expertise barrier from PhD â†’ High School
   â€¢ Cuts development time from weeks â†’ hours
   â€¢ Provides interpretable quality metrics
   â€¢ Enables domain experts to directly improve models
   â€¢ Scales AI development across organizations
    """)
    
    # Show a simple template that a non-technical user could create
    simple_template = Template("customer_support")
    simple_template.description = "Evaluate customer support responses for helpfulness and professionalism"
    simple_template.add_rule("contains_phrase", 2.0, phrase="help")
    simple_template.add_rule("contains_phrase", 1.5, phrase="understand")
    simple_template.add_rule("sentiment_positive", 2.0)
    simple_template.add_rule("word_count", 1.0, min_words=20, max_words=150)
    
    print("\nğŸ“ EXAMPLE: Non-Technical User Creates Customer Support Rubric")
    print("   (No coding required - just YAML configuration)")
    
    sample_response = """
    I understand your frustration with the billing issue. Let me help you resolve this quickly.
    I've reviewed your account and can see the problem. I'll process a refund immediately
    and ensure this doesn't happen again. Is there anything else I can help you with today?
    """
    
    result = simple_template.evaluate_detailed(sample_response)
    print(f"\nğŸ¯ Instant Feedback: {result['total_score']:.3f}")
    print("   âœ… Contains 'help' and 'understand'")
    print("   âœ… Positive, professional tone")
    print("   âœ… Appropriate length")
    print("\nğŸ’¡ This rubric can now train AI models to write better support responses!")


def main():
    """Run the advanced ClarityAI demonstration."""
    print("ğŸš€ ClarityAI Advanced Demonstration")
    print("   Making Fine-Tuning Accessible with Enterprise-Grade Rubrics")
    
    try:
        demo_fine_tuning_accessibility()
        demo_academic_paper()
        demo_code_review()
        
        print_separator("SUMMARY")
        print("""
ğŸ‰ ClarityAI transforms fine-tuning from a complex ML engineering task into an
   intuitive rubric design process that domain experts can master.

ğŸ”‘ Key Advantages:
   â€¢ Explainable AI with detailed reasoning
   â€¢ Actionable feedback for continuous improvement  
   â€¢ Accessible to non-technical domain experts
   â€¢ Enterprise-grade evaluation sophistication
   â€¢ Seamless integration with existing workflows

ğŸš€ Ready to revolutionize your AI development process?
   Start with: streamlit run app.py
        """)
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ Demo interrupted by user")
    except Exception as e:
        print(f"\nâŒ Demo error: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()