{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.0,
  "eval_steps": 500,
  "global_step": 105,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.04938271604938271,
      "grad_norm": 105.09589385986328,
      "learning_rate": 0.0001,
      "loss": 8.5477,
      "step": 1
    },
    {
      "epoch": 0.09876543209876543,
      "grad_norm": 116.53986358642578,
      "learning_rate": 9.904761904761905e-05,
      "loss": 10.5744,
      "step": 2
    },
    {
      "epoch": 0.14814814814814814,
      "grad_norm": 80.38766479492188,
      "learning_rate": 9.80952380952381e-05,
      "loss": 7.8171,
      "step": 3
    },
    {
      "epoch": 0.19753086419753085,
      "grad_norm": 82.41041564941406,
      "learning_rate": 9.714285714285715e-05,
      "loss": 6.7938,
      "step": 4
    },
    {
      "epoch": 0.24691358024691357,
      "grad_norm": 81.698486328125,
      "learning_rate": 9.61904761904762e-05,
      "loss": 6.4701,
      "step": 5
    },
    {
      "epoch": 0.2962962962962963,
      "grad_norm": 83.93193054199219,
      "learning_rate": 9.523809523809524e-05,
      "loss": 7.7414,
      "step": 6
    },
    {
      "epoch": 0.345679012345679,
      "grad_norm": 78.66659545898438,
      "learning_rate": 9.428571428571429e-05,
      "loss": 6.8788,
      "step": 7
    },
    {
      "epoch": 0.3950617283950617,
      "grad_norm": 59.229270935058594,
      "learning_rate": 9.333333333333334e-05,
      "loss": 5.6813,
      "step": 8
    },
    {
      "epoch": 0.4444444444444444,
      "grad_norm": 46.984642028808594,
      "learning_rate": 9.238095238095239e-05,
      "loss": 5.4686,
      "step": 9
    },
    {
      "epoch": 0.49382716049382713,
      "grad_norm": 48.978309631347656,
      "learning_rate": 9.142857142857143e-05,
      "loss": 5.2636,
      "step": 10
    },
    {
      "epoch": 0.5432098765432098,
      "grad_norm": 42.921974182128906,
      "learning_rate": 9.047619047619048e-05,
      "loss": 4.6383,
      "step": 11
    },
    {
      "epoch": 0.5925925925925926,
      "grad_norm": 45.37873840332031,
      "learning_rate": 8.952380952380953e-05,
      "loss": 4.6584,
      "step": 12
    },
    {
      "epoch": 0.6419753086419753,
      "grad_norm": 32.40669631958008,
      "learning_rate": 8.857142857142857e-05,
      "loss": 4.1105,
      "step": 13
    },
    {
      "epoch": 0.691358024691358,
      "grad_norm": 33.73479461669922,
      "learning_rate": 8.761904761904762e-05,
      "loss": 3.6625,
      "step": 14
    },
    {
      "epoch": 0.7407407407407407,
      "grad_norm": 30.232404708862305,
      "learning_rate": 8.666666666666667e-05,
      "loss": 3.9861,
      "step": 15
    },
    {
      "epoch": 0.7901234567901234,
      "grad_norm": 33.13549041748047,
      "learning_rate": 8.571428571428571e-05,
      "loss": 3.55,
      "step": 16
    },
    {
      "epoch": 0.8395061728395061,
      "grad_norm": 24.0950870513916,
      "learning_rate": 8.476190476190477e-05,
      "loss": 2.935,
      "step": 17
    },
    {
      "epoch": 0.8888888888888888,
      "grad_norm": 32.31689453125,
      "learning_rate": 8.380952380952382e-05,
      "loss": 3.4566,
      "step": 18
    },
    {
      "epoch": 0.9382716049382716,
      "grad_norm": 23.78271484375,
      "learning_rate": 8.285714285714287e-05,
      "loss": 3.0212,
      "step": 19
    },
    {
      "epoch": 0.9876543209876543,
      "grad_norm": 29.98074722290039,
      "learning_rate": 8.19047619047619e-05,
      "loss": 3.1171,
      "step": 20
    },
    {
      "epoch": 1.0,
      "grad_norm": 62.46697998046875,
      "learning_rate": 8.095238095238096e-05,
      "loss": 3.202,
      "step": 21
    },
    {
      "epoch": 1.0493827160493827,
      "grad_norm": 21.468395233154297,
      "learning_rate": 8e-05,
      "loss": 2.5976,
      "step": 22
    },
    {
      "epoch": 1.0987654320987654,
      "grad_norm": 29.659900665283203,
      "learning_rate": 7.904761904761905e-05,
      "loss": 2.7806,
      "step": 23
    },
    {
      "epoch": 1.1481481481481481,
      "grad_norm": 28.62128257751465,
      "learning_rate": 7.80952380952381e-05,
      "loss": 2.8309,
      "step": 24
    },
    {
      "epoch": 1.1975308641975309,
      "grad_norm": 27.22373390197754,
      "learning_rate": 7.714285714285715e-05,
      "loss": 2.5228,
      "step": 25
    },
    {
      "epoch": 1.2469135802469136,
      "grad_norm": 25.014448165893555,
      "learning_rate": 7.619047619047618e-05,
      "loss": 2.3753,
      "step": 26
    },
    {
      "epoch": 1.2962962962962963,
      "grad_norm": 25.708038330078125,
      "learning_rate": 7.523809523809524e-05,
      "loss": 2.4887,
      "step": 27
    },
    {
      "epoch": 1.345679012345679,
      "grad_norm": 26.299373626708984,
      "learning_rate": 7.428571428571429e-05,
      "loss": 2.1932,
      "step": 28
    },
    {
      "epoch": 1.3950617283950617,
      "grad_norm": 25.57619857788086,
      "learning_rate": 7.333333333333333e-05,
      "loss": 2.0737,
      "step": 29
    },
    {
      "epoch": 1.4444444444444444,
      "grad_norm": 24.791751861572266,
      "learning_rate": 7.238095238095238e-05,
      "loss": 2.4203,
      "step": 30
    },
    {
      "epoch": 1.4938271604938271,
      "grad_norm": 21.208478927612305,
      "learning_rate": 7.142857142857143e-05,
      "loss": 1.9767,
      "step": 31
    },
    {
      "epoch": 1.5432098765432098,
      "grad_norm": 31.609899520874023,
      "learning_rate": 7.047619047619048e-05,
      "loss": 2.5956,
      "step": 32
    },
    {
      "epoch": 1.5925925925925926,
      "grad_norm": 29.899606704711914,
      "learning_rate": 6.952380952380952e-05,
      "loss": 2.2553,
      "step": 33
    },
    {
      "epoch": 1.6419753086419753,
      "grad_norm": 25.53118324279785,
      "learning_rate": 6.857142857142858e-05,
      "loss": 2.4584,
      "step": 34
    },
    {
      "epoch": 1.691358024691358,
      "grad_norm": 27.17910385131836,
      "learning_rate": 6.761904761904763e-05,
      "loss": 2.233,
      "step": 35
    },
    {
      "epoch": 1.7407407407407407,
      "grad_norm": 20.17732048034668,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.2042,
      "step": 36
    },
    {
      "epoch": 1.7901234567901234,
      "grad_norm": 26.37693977355957,
      "learning_rate": 6.571428571428571e-05,
      "loss": 2.1516,
      "step": 37
    },
    {
      "epoch": 1.8395061728395061,
      "grad_norm": 23.503013610839844,
      "learning_rate": 6.476190476190477e-05,
      "loss": 2.2605,
      "step": 38
    },
    {
      "epoch": 1.8888888888888888,
      "grad_norm": 26.94330596923828,
      "learning_rate": 6.38095238095238e-05,
      "loss": 2.0572,
      "step": 39
    },
    {
      "epoch": 1.9382716049382716,
      "grad_norm": 31.93364143371582,
      "learning_rate": 6.285714285714286e-05,
      "loss": 2.3155,
      "step": 40
    },
    {
      "epoch": 1.9876543209876543,
      "grad_norm": 29.544307708740234,
      "learning_rate": 6.19047619047619e-05,
      "loss": 2.2081,
      "step": 41
    },
    {
      "epoch": 2.0,
      "grad_norm": 60.75894546508789,
      "learning_rate": 6.0952380952380964e-05,
      "loss": 2.329,
      "step": 42
    },
    {
      "epoch": 2.049382716049383,
      "grad_norm": 28.0158634185791,
      "learning_rate": 6e-05,
      "loss": 2.2768,
      "step": 43
    },
    {
      "epoch": 2.0987654320987654,
      "grad_norm": 21.95496368408203,
      "learning_rate": 5.904761904761905e-05,
      "loss": 1.726,
      "step": 44
    },
    {
      "epoch": 2.148148148148148,
      "grad_norm": 17.699907302856445,
      "learning_rate": 5.8095238095238104e-05,
      "loss": 1.6048,
      "step": 45
    },
    {
      "epoch": 2.197530864197531,
      "grad_norm": 21.209108352661133,
      "learning_rate": 5.714285714285714e-05,
      "loss": 1.8969,
      "step": 46
    },
    {
      "epoch": 2.246913580246914,
      "grad_norm": 21.662982940673828,
      "learning_rate": 5.619047619047619e-05,
      "loss": 2.129,
      "step": 47
    },
    {
      "epoch": 2.2962962962962963,
      "grad_norm": 28.575796127319336,
      "learning_rate": 5.5238095238095244e-05,
      "loss": 2.4601,
      "step": 48
    },
    {
      "epoch": 2.3456790123456788,
      "grad_norm": 22.010726928710938,
      "learning_rate": 5.428571428571428e-05,
      "loss": 1.7752,
      "step": 49
    },
    {
      "epoch": 2.3950617283950617,
      "grad_norm": 27.870840072631836,
      "learning_rate": 5.333333333333333e-05,
      "loss": 1.8915,
      "step": 50
    },
    {
      "epoch": 2.4444444444444446,
      "grad_norm": 27.08025360107422,
      "learning_rate": 5.2380952380952384e-05,
      "loss": 2.1224,
      "step": 51
    },
    {
      "epoch": 2.493827160493827,
      "grad_norm": 18.43897819519043,
      "learning_rate": 5.142857142857143e-05,
      "loss": 1.4705,
      "step": 52
    },
    {
      "epoch": 2.5432098765432096,
      "grad_norm": 21.508363723754883,
      "learning_rate": 5.047619047619048e-05,
      "loss": 1.964,
      "step": 53
    },
    {
      "epoch": 2.5925925925925926,
      "grad_norm": 25.597917556762695,
      "learning_rate": 4.9523809523809525e-05,
      "loss": 1.9516,
      "step": 54
    },
    {
      "epoch": 2.6419753086419755,
      "grad_norm": 23.521163940429688,
      "learning_rate": 4.8571428571428576e-05,
      "loss": 2.0941,
      "step": 55
    },
    {
      "epoch": 2.691358024691358,
      "grad_norm": 24.37565040588379,
      "learning_rate": 4.761904761904762e-05,
      "loss": 1.6677,
      "step": 56
    },
    {
      "epoch": 2.7407407407407405,
      "grad_norm": 27.77042007446289,
      "learning_rate": 4.666666666666667e-05,
      "loss": 1.6551,
      "step": 57
    },
    {
      "epoch": 2.7901234567901234,
      "grad_norm": 19.707189559936523,
      "learning_rate": 4.5714285714285716e-05,
      "loss": 1.6038,
      "step": 58
    },
    {
      "epoch": 2.8395061728395063,
      "grad_norm": 25.02378273010254,
      "learning_rate": 4.476190476190477e-05,
      "loss": 1.9246,
      "step": 59
    },
    {
      "epoch": 2.888888888888889,
      "grad_norm": 24.27564811706543,
      "learning_rate": 4.380952380952381e-05,
      "loss": 1.698,
      "step": 60
    },
    {
      "epoch": 2.9382716049382713,
      "grad_norm": 20.657995223999023,
      "learning_rate": 4.2857142857142856e-05,
      "loss": 1.6165,
      "step": 61
    },
    {
      "epoch": 2.9876543209876543,
      "grad_norm": 27.376157760620117,
      "learning_rate": 4.190476190476191e-05,
      "loss": 1.9057,
      "step": 62
    },
    {
      "epoch": 3.0,
      "grad_norm": 62.7255744934082,
      "learning_rate": 4.095238095238095e-05,
      "loss": 1.9272,
      "step": 63
    },
    {
      "epoch": 3.049382716049383,
      "grad_norm": 21.980636596679688,
      "learning_rate": 4e-05,
      "loss": 1.4035,
      "step": 64
    },
    {
      "epoch": 3.0987654320987654,
      "grad_norm": 19.93733787536621,
      "learning_rate": 3.904761904761905e-05,
      "loss": 1.5215,
      "step": 65
    },
    {
      "epoch": 3.148148148148148,
      "grad_norm": 18.222366333007812,
      "learning_rate": 3.809523809523809e-05,
      "loss": 1.3808,
      "step": 66
    },
    {
      "epoch": 3.197530864197531,
      "grad_norm": 19.672826766967773,
      "learning_rate": 3.7142857142857143e-05,
      "loss": 1.5063,
      "step": 67
    },
    {
      "epoch": 3.246913580246914,
      "grad_norm": 20.192174911499023,
      "learning_rate": 3.619047619047619e-05,
      "loss": 1.5362,
      "step": 68
    },
    {
      "epoch": 3.2962962962962963,
      "grad_norm": 21.818538665771484,
      "learning_rate": 3.523809523809524e-05,
      "loss": 1.4941,
      "step": 69
    },
    {
      "epoch": 3.3456790123456788,
      "grad_norm": 31.94257926940918,
      "learning_rate": 3.428571428571429e-05,
      "loss": 1.7334,
      "step": 70
    },
    {
      "epoch": 3.3950617283950617,
      "grad_norm": 21.439773559570312,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 1.8285,
      "step": 71
    },
    {
      "epoch": 3.4444444444444446,
      "grad_norm": 23.770938873291016,
      "learning_rate": 3.2380952380952386e-05,
      "loss": 1.6258,
      "step": 72
    },
    {
      "epoch": 3.493827160493827,
      "grad_norm": 21.181068420410156,
      "learning_rate": 3.142857142857143e-05,
      "loss": 1.545,
      "step": 73
    },
    {
      "epoch": 3.5432098765432096,
      "grad_norm": 28.82175064086914,
      "learning_rate": 3.0476190476190482e-05,
      "loss": 1.8717,
      "step": 74
    },
    {
      "epoch": 3.5925925925925926,
      "grad_norm": 15.37929630279541,
      "learning_rate": 2.9523809523809526e-05,
      "loss": 1.1104,
      "step": 75
    },
    {
      "epoch": 3.6419753086419755,
      "grad_norm": 27.213430404663086,
      "learning_rate": 2.857142857142857e-05,
      "loss": 1.839,
      "step": 76
    },
    {
      "epoch": 3.691358024691358,
      "grad_norm": 21.056262969970703,
      "learning_rate": 2.7619047619047622e-05,
      "loss": 1.5812,
      "step": 77
    },
    {
      "epoch": 3.7407407407407405,
      "grad_norm": 29.898529052734375,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 1.7186,
      "step": 78
    },
    {
      "epoch": 3.7901234567901234,
      "grad_norm": 25.572860717773438,
      "learning_rate": 2.5714285714285714e-05,
      "loss": 1.817,
      "step": 79
    },
    {
      "epoch": 3.8395061728395063,
      "grad_norm": 26.279300689697266,
      "learning_rate": 2.4761904761904762e-05,
      "loss": 1.7325,
      "step": 80
    },
    {
      "epoch": 3.888888888888889,
      "grad_norm": 21.322420120239258,
      "learning_rate": 2.380952380952381e-05,
      "loss": 1.6435,
      "step": 81
    },
    {
      "epoch": 3.9382716049382713,
      "grad_norm": 28.322383880615234,
      "learning_rate": 2.2857142857142858e-05,
      "loss": 1.4565,
      "step": 82
    },
    {
      "epoch": 3.9876543209876543,
      "grad_norm": 26.102310180664062,
      "learning_rate": 2.1904761904761906e-05,
      "loss": 1.6504,
      "step": 83
    },
    {
      "epoch": 4.0,
      "grad_norm": 52.4014778137207,
      "learning_rate": 2.0952380952380954e-05,
      "loss": 1.6295,
      "step": 84
    },
    {
      "epoch": 4.049382716049383,
      "grad_norm": 30.599830627441406,
      "learning_rate": 2e-05,
      "loss": 1.6878,
      "step": 85
    },
    {
      "epoch": 4.098765432098766,
      "grad_norm": 27.015274047851562,
      "learning_rate": 1.9047619047619046e-05,
      "loss": 1.5525,
      "step": 86
    },
    {
      "epoch": 4.148148148148148,
      "grad_norm": 16.634687423706055,
      "learning_rate": 1.8095238095238094e-05,
      "loss": 1.2724,
      "step": 87
    },
    {
      "epoch": 4.197530864197531,
      "grad_norm": 24.4459171295166,
      "learning_rate": 1.7142857142857145e-05,
      "loss": 1.5739,
      "step": 88
    },
    {
      "epoch": 4.246913580246914,
      "grad_norm": 21.38843536376953,
      "learning_rate": 1.6190476190476193e-05,
      "loss": 1.387,
      "step": 89
    },
    {
      "epoch": 4.296296296296296,
      "grad_norm": 28.943593978881836,
      "learning_rate": 1.5238095238095241e-05,
      "loss": 1.7823,
      "step": 90
    },
    {
      "epoch": 4.345679012345679,
      "grad_norm": 26.224260330200195,
      "learning_rate": 1.4285714285714285e-05,
      "loss": 1.551,
      "step": 91
    },
    {
      "epoch": 4.395061728395062,
      "grad_norm": 21.31294059753418,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 1.4903,
      "step": 92
    },
    {
      "epoch": 4.444444444444445,
      "grad_norm": 17.92298698425293,
      "learning_rate": 1.2380952380952381e-05,
      "loss": 1.6327,
      "step": 93
    },
    {
      "epoch": 4.493827160493828,
      "grad_norm": 27.498159408569336,
      "learning_rate": 1.1428571428571429e-05,
      "loss": 1.683,
      "step": 94
    },
    {
      "epoch": 4.54320987654321,
      "grad_norm": 21.126819610595703,
      "learning_rate": 1.0476190476190477e-05,
      "loss": 1.7165,
      "step": 95
    },
    {
      "epoch": 4.592592592592593,
      "grad_norm": 20.963050842285156,
      "learning_rate": 9.523809523809523e-06,
      "loss": 1.4525,
      "step": 96
    },
    {
      "epoch": 4.6419753086419755,
      "grad_norm": 21.701250076293945,
      "learning_rate": 8.571428571428573e-06,
      "loss": 1.3896,
      "step": 97
    },
    {
      "epoch": 4.6913580246913575,
      "grad_norm": 23.15957260131836,
      "learning_rate": 7.6190476190476205e-06,
      "loss": 1.5697,
      "step": 98
    },
    {
      "epoch": 4.7407407407407405,
      "grad_norm": 23.04268455505371,
      "learning_rate": 6.666666666666667e-06,
      "loss": 1.4589,
      "step": 99
    },
    {
      "epoch": 4.790123456790123,
      "grad_norm": 21.188398361206055,
      "learning_rate": 5.7142857142857145e-06,
      "loss": 1.42,
      "step": 100
    },
    {
      "epoch": 4.839506172839506,
      "grad_norm": 26.166038513183594,
      "learning_rate": 4.7619047619047615e-06,
      "loss": 1.4062,
      "step": 101
    },
    {
      "epoch": 4.888888888888889,
      "grad_norm": 20.472002029418945,
      "learning_rate": 3.8095238095238102e-06,
      "loss": 1.5094,
      "step": 102
    },
    {
      "epoch": 4.938271604938271,
      "grad_norm": 20.500816345214844,
      "learning_rate": 2.8571428571428573e-06,
      "loss": 1.3329,
      "step": 103
    },
    {
      "epoch": 4.987654320987654,
      "grad_norm": 23.405488967895508,
      "learning_rate": 1.9047619047619051e-06,
      "loss": 1.5654,
      "step": 104
    },
    {
      "epoch": 5.0,
      "grad_norm": 37.03391647338867,
      "learning_rate": 9.523809523809526e-07,
      "loss": 1.1424,
      "step": 105
    }
  ],
  "logging_steps": 1,
  "max_steps": 105,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 5,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 105823272960000.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
